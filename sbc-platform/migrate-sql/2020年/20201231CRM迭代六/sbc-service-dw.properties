# ----------------------------------------
# application info
# ----------------------------------------
info.app.name=sbc-service-dw
info.app.description=sbc-service-dw platform
info.build.artifact=@project.artifactId@
info.build.version=@project.version@
info.app.encoding=@project.build.sourceEncoding@
# ----------------------------------------
# actuator settings
# ----------------------------------------
manager.mode = sbc
management.server.port=8841
management.endpoints.enabled-by-default=false
management.endpoints.web.base-path=/act
management.endpoint.info.enabled=true
management.endpoint.health.enabled=true
management.endpoint.mappings.enabled=true
management.endpoint.env.enabled=true
management.endpoints.web.exposure.include=env,health,info,mappings
management.endpoint.health.show-details=always

spring.cloud.nacos.config.enabled = false
spring.application.name=sbc-service-dw
spring.cloud.nacos.discovery.server-addr=172.19.25.12:8848
spring.cloud.nacos.discovery.service=sbc-service-dw

# ----------------------------------------
# i18n
# ----------------------------------------
spring.messages.basename=i18n/ResultCode
spring.messages.cache-duration=5

# ----------------------------------------
# log config
# ----------------------------------------
logging.level.root=INFO
logging.level.com.wanmi=DEBUG
logging.level.org.springframework=WARN
logging.level.com.alibaba.nacos.client=ERROR
logging.level.org.apache.phoenix.query.ConnectionQueryServicesImpl=ERROR
logging.config=classpath:logback.xml

# ----------------------------------------
# server settings
# ----------------------------------------
server.port=8840
server.servlet.session.timeout=50
server.tomcat.max-threads=200
server.error.whitelabel.enabled=false
server.error.include-stacktrace=ALWAYS
server.tomcat.basedir=${user.home}/htdocs/data/sbc/dw/tmp

spring.jpa.show-sql=true
spring.jpa.open-in-view=true

async.thread-name-prefix=dw-async-executor-
async.corePoolSize=10
async.maxPoolSize=50
async.queueCapacity=2048

# -------------------------
# database settings
# -------------------------
spring.datasource.hikari.driver-class-name=com.mysql.cj.jdbc.Driver
spring.datasource.hikari.jdbc-url=jdbc:mysql://172.19.25.97:3306/sbc-crm?characterEncoding=UTF-8&&zeroDateTimeBehavior=convertToNull&autoReconnect=true&failOverReadOnly=false&connectTimeout=0&serverTimezone=Asia/Shanghai&allowMultiQueries=true
spring.datasource.hikari.username=root
spring.datasource.hikari.password=Wmi@2019
spring.datasource.hikari.minimum-idle=5
spring.datasource.hikari.maximum-pool-size=15
spring.datasource.hikari.idle-timeout=30000
spring.datasource.hikari.max-lifetime=60000
spring.datasource.hikari.connection-timeout=30000
#phoenix数据源
spring.datasource2.phoenix.url=jdbc:phoenix:hdp-master.test.com,hdp-node1.test.com,hdp-node2.test.com:2181
spring.datasource2.phoenix.driver-class-name=org.apache.phoenix.jdbc.PhoenixDriver
spring.datasource2.phoenix.username=
spring.datasource2.phoenix.password=
spring.datasource2.phoenix.default-auto-commit=true
# ----------------------------------------
# json message converter
# ----------------------------------------
spring.jackson.date-format=yyyy-MM-dd HH:mm:ss
spring.jackson.default-property-inclusion=non_null

# ----------------------------------------
# mybatis config
# ----------------------------------------
mybatis.mapper-locations=classpath*:META-INF/mybatis/mapper/*Mapper.xml
#mysql\u5BF9\u5E94\u7684schema\uFF0C\u8BBE\u5B9A\u5168\u5C40\u53D8\u91CF\uFF0Cmybatis xml \u6587\u4EF6\u76F4\u63A5\u901A\u8FC7${aresDb}\u83B7\u53D6
mybatis.configuration.variables.aresDB=`s2b_statistics`.
mybatis.configuration.variables.crmDB = `sbc-crm`.



#pagehelper
pagehelper.helperDialect=mysql
pagehelper.reasonable=true
pagehelper.supportMethodsArguments=true
pagehelper.params=count=countSql


### xxl-job admin address list, such as "http://address" or "http://address01,http://address02"
xxl.job.admin.addresses=http://172.19.25.12:8990/xxl-job-admin

### xxl-job executor address
xxl.job.executor.appname=xxl-job-dw
xxl.job.executor.ip=
xxl.job.executor.port=9999

### xxl-job, access token
xxl.job.accessToken=b5f7ed32b2e24789bdce1308afcafebe

### xxl-job log path
xxl.job.executor.logpath=./log/jobhandler
### xxl-job log retention days
xxl.job.executor.logretentiondays=-1

# cli  beeline spark
hive.config.clientMode = cli
hive.config.useRemoteCli = true
hive.config.remoteCliHostname = 172.19.25.29
hive.config.remoteCliPort = 22
hive.config.remoteCliUsername = hive
hive.config.remoteCliPassword = 111111
hive.config.beelineParams = -n hive -p hive --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u 'jdbc:hive2://hdp-node1:10000'
hive.config.hadoopJobUrl = http://hdp-node1:19888
hive.script.file = classpath:hive_etl.sql


#-----------------------------------------
# hbase config
# ----------------------------------------
hbase.master= hdp-master.test.com:16000
hbase.zookeeper.quorum= hdp-master.test.com,hdp-node1.test.com,hdp-node2.test.com
hbase.zookeeper.property.clientPort= 2181
hbase.zookeeper.znode.parent= /hbase
hbase.split.info= 1,2,3
#  info: 1,2,3,4,5,6,7,8,9,A,B,C,D,E,F  #表预切分
#是否采用压缩
hbase.isCompression= false